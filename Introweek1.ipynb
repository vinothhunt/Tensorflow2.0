{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introweek1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDa4DNTU6n27",
        "colab_type": "text"
      },
      "source": [
        "Basic Tensorfow Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zMyQx9-RV6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "403fa941-7155-479f-9a0e-e59ff094e2cd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer='sgd',loss='mean_squared_error')\n",
        "xs=np.array([1.0,1.5,2.0,2.5,3.0,3.5,],dtype=float)\n",
        "ys=np.array([1.0,1.5,2.0,2.5,3.0,3.5,],dtype=float)\n",
        "model.fit(xs,ys,epochs=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.2955\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.7244\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2957\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9739\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 923us/step - loss: 0.7324\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5511\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4150\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3129\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2363\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1787\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1355\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1031\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0787\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0605\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0467\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0364\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0287\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0229\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0185\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0109\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0095\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0084\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0076\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0070\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0062\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0059\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0057\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0056\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0054\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0053\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0052\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0051\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0051\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0050\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0050\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0050\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0049\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0049\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0049\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0049\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0048\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0048\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0048\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0048\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0047\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0047\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0047\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0046\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0046\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0046\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0045\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0045\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0045\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0045\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0045\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0045\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0043\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0043\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0043\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0043\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0043\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0041\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0041\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0041\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0041\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0041\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0031\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0031\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0027\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0027\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0027\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0026\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0026\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0026\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0025\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0017\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0015\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0014\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0014\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0014\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0014\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 986us/step - loss: 0.0013\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0010\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0010\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0010\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.9877e-04\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.9442e-04\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.9008e-04\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.8576e-04\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.8147e-04\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.7719e-04\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.7293e-04\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.6869e-04\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.6446e-04\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.6026e-04\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.5607e-04\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.5190e-04\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.4775e-04\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.4362e-04\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.3951e-04\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.3541e-04\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.3133e-04\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.2727e-04\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.2323e-04\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.1920e-04\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.1520e-04\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.1121e-04\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.0723e-04\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.0328e-04\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.9934e-04\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.9542e-04\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.9151e-04\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.8763e-04\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.8376e-04\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.7990e-04\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.7607e-04\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.7225e-04\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.6844e-04\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.6466e-04\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.6089e-04\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.5714e-04\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.5340e-04\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.4968e-04\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.4598e-04\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.4228e-04\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3861e-04\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3496e-04\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3132e-04\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.2769e-04\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.2408e-04\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.2049e-04\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.1692e-04\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.1335e-04\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 8.0980e-04\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.0628e-04\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.0276e-04\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.9926e-04\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.9577e-04\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.9231e-04\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.8885e-04\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.8541e-04\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.8199e-04\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.7858e-04\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.7518e-04\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.7180e-04\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.6844e-04\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.6509e-04\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.6176e-04\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.5843e-04\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.5513e-04\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.5184e-04\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.4856e-04\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.4529e-04\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.4204e-04\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.3881e-04\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.3558e-04\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 7.3238e-04\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.2919e-04\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.2601e-04\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.2284e-04\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.1969e-04\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.1655e-04\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.1343e-04\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.1032e-04\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 7.0722e-04\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.0414e-04\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.0107e-04\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.9801e-04\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.9497e-04\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.9194e-04\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.8892e-04\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.8592e-04\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.8293e-04\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.7995e-04\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.7699e-04\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.7403e-04\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.7109e-04\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.6817e-04\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.6526e-04\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.6236e-04\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.5947e-04\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.5659e-04\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.5373e-04\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.5088e-04\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.4804e-04\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.4522e-04\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.4240e-04\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.3960e-04\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.3682e-04\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.3404e-04\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.3127e-04\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.2852e-04\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.2578e-04\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.2305e-04\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.2034e-04\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.1763e-04\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.1494e-04\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.1226e-04\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.0959e-04\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.0693e-04\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.0429e-04\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.0165e-04\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.9903e-04\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.9642e-04\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.9382e-04\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.9123e-04\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.8865e-04\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.8608e-04\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.8353e-04\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.8098e-04\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.7845e-04\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.7593e-04\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.7342e-04\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.7092e-04\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.6843e-04\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.6595e-04\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.6348e-04\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.6102e-04\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.5858e-04\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.5615e-04\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.5372e-04\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.5131e-04\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.4890e-04\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.4651e-04\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.4413e-04\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.4175e-04\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.3939e-04\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3704e-04\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3470e-04\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3237e-04\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3004e-04\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.2774e-04\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.2543e-04\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.2315e-04\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.2086e-04\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.1859e-04\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.1633e-04\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.1408e-04\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.1184e-04\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.0961e-04\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.0739e-04\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.0517e-04\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.0297e-04\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.0078e-04\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.9860e-04\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.9642e-04\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.9426e-04\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.9211e-04\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.8996e-04\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.8782e-04\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.8570e-04\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.8358e-04\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.8147e-04\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.7937e-04\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.7728e-04\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.7520e-04\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.7313e-04\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.7107e-04\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.6901e-04\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.6697e-04\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.6493e-04\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.6290e-04\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.6089e-04\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5888e-04\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.5688e-04\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5488e-04\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5290e-04\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5092e-04\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4896e-04\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4700e-04\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4505e-04\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4311e-04\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4118e-04\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.3926e-04\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.3734e-04\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.3544e-04\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.3354e-04\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.3165e-04\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2977e-04\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2789e-04\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2603e-04\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2417e-04\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2232e-04\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.2048e-04\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1864e-04\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1682e-04\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1500e-04\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1319e-04\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1139e-04\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.0960e-04\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.0781e-04\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.0603e-04\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.0426e-04\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.0250e-04\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 895us/step - loss: 4.0075e-04\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.9900e-04\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.9726e-04\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.9553e-04\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 892us/step - loss: 3.9380e-04\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 922us/step - loss: 3.9209e-04\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.9038e-04\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.8868e-04\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.8698e-04\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.8529e-04\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.8361e-04\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.8194e-04\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.8027e-04\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7862e-04\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7697e-04\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.7532e-04\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7369e-04\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7206e-04\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7044e-04\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.6882e-04\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.6721e-04\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.6561e-04\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.6402e-04\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.6243e-04\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.6085e-04\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.5928e-04\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.5771e-04\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.5615e-04\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.5460e-04\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.5305e-04\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.5151e-04\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4998e-04\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4846e-04\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4694e-04\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4542e-04\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4392e-04\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.4242e-04\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4092e-04\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.3944e-04\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.3796e-04\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.3648e-04\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.3502e-04\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.3356e-04\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.3210e-04\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.3066e-04\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2921e-04\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2778e-04\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2635e-04\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2493e-04\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2351e-04\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2210e-04\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2069e-04\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.1930e-04\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.1790e-04\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.1652e-04\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.1514e-04\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.1376e-04\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.1240e-04\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.1104e-04\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0968e-04\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0833e-04\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0698e-04\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.0565e-04\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0431e-04\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0299e-04\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.0167e-04\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0035e-04\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.9904e-04\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9774e-04\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9644e-04\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9515e-04\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.9386e-04\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9258e-04\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9130e-04\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9003e-04\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8877e-04\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8751e-04\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8626e-04\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8501e-04\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8376e-04\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8253e-04\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8130e-04\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8007e-04\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7885e-04\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7763e-04\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7642e-04\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7522e-04\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7402e-04\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7282e-04\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7163e-04\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.7045e-04\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.6927e-04\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6810e-04\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6693e-04\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6576e-04\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6460e-04\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6345e-04\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6230e-04\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6116e-04\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6002e-04\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5889e-04\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5776e-04\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.5663e-04\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.5552e-04\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5440e-04\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.5329e-04\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5219e-04\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5109e-04\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.4999e-04\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.4890e-04\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.4782e-04\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4674e-04\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4566e-04\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.4459e-04\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4353e-04\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4246e-04\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4141e-04\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4035e-04\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3930e-04\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3826e-04\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3722e-04\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3619e-04\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3516e-04\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.3413e-04\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.3311e-04\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3210e-04\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.3109e-04\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3008e-04\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.2908e-04\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.2808e-04\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.2708e-04\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.2609e-04\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.2511e-04\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.2412e-04\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.2315e-04\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.2217e-04\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.2120e-04\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.2024e-04\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.1928e-04\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.1832e-04\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1737e-04\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1643e-04\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1548e-04\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1454e-04\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1361e-04\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1268e-04\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.1175e-04\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1083e-04\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.0991e-04\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.0899e-04\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.0808e-04\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.0717e-04\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.0627e-04\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.0537e-04\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 910us/step - loss: 2.0448e-04\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 937us/step - loss: 2.0358e-04\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.0270e-04\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.0181e-04\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.0093e-04\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 948us/step - loss: 2.0006e-04\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9918e-04\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9832e-04\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9745e-04\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9659e-04\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9573e-04\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9488e-04\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9403e-04\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9318e-04\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9234e-04\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9150e-04\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9067e-04\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8984e-04\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8901e-04\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8819e-04\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8737e-04\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8655e-04\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8573e-04\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.8492e-04\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.8412e-04\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.8332e-04\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.8252e-04\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8172e-04\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8093e-04\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8014e-04\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7936e-04\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7857e-04\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.7779e-04\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7702e-04\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7625e-04\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7548e-04\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7471e-04\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.7395e-04\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7319e-04\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7244e-04\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7169e-04\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7094e-04\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7019e-04\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6945e-04\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6871e-04\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6798e-04\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6725e-04\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6652e-04\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.6579e-04\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6507e-04\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.6435e-04\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6363e-04\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6292e-04\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6221e-04\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6150e-04\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6079e-04\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6009e-04\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5940e-04\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5870e-04\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5801e-04\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.5732e-04\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.5664e-04\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.5595e-04\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5527e-04\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5460e-04\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5392e-04\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.5325e-04\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.5258e-04\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5192e-04\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5125e-04\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5059e-04\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4994e-04\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4928e-04\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4863e-04\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4799e-04\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4734e-04\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4670e-04\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4606e-04\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4542e-04\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4479e-04\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4416e-04\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4353e-04\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.4290e-04\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4228e-04\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4166e-04\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4104e-04\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4043e-04\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3981e-04\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3920e-04\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3860e-04\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3799e-04\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3739e-04\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3679e-04\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3620e-04\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3560e-04\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3501e-04\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3442e-04\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3384e-04\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3325e-04\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3267e-04\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3209e-04\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3152e-04\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3095e-04\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3037e-04\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2981e-04\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2924e-04\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2868e-04\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2812e-04\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2756e-04\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2700e-04\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2645e-04\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2590e-04\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2535e-04\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2480e-04\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2426e-04\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2371e-04\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2318e-04\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2264e-04\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2210e-04\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2157e-04\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2104e-04\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2051e-04\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1999e-04\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1946e-04\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1894e-04\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1843e-04\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1791e-04\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1739e-04\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1688e-04\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1637e-04\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1587e-04\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1536e-04\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1486e-04\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1436e-04\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1386e-04\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1336e-04\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1287e-04\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1238e-04\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1189e-04\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1140e-04\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1091e-04\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1043e-04\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0995e-04\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0947e-04\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0899e-04\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0852e-04\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0804e-04\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0757e-04\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0710e-04\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0664e-04\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0617e-04\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0571e-04\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0525e-04\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0479e-04\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0433e-04\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0388e-04\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0342e-04\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0297e-04\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0252e-04\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0208e-04\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0163e-04\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0119e-04\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0075e-04\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0031e-04\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.9870e-05\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.9435e-05\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.9001e-05\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.8570e-05\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.8140e-05\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.7713e-05\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.7287e-05\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.6862e-05\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.6440e-05\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.6019e-05\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.5601e-05\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.5184e-05\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.4768e-05\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.4355e-05\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.3944e-05\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.3535e-05\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.3127e-05\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.2721e-05\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.2317e-05\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.1914e-05\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.1513e-05\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.1114e-05\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.0718e-05\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.0322e-05\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.9928e-05\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.9535e-05\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.9145e-05\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.8756e-05\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.8370e-05\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.7985e-05\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.7601e-05\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.7219e-05\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.6839e-05\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.6461e-05\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.6083e-05\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.5707e-05\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.5334e-05\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.4963e-05\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.4591e-05\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.4223e-05\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3856e-05\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3491e-05\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3126e-05\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.2764e-05\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.2403e-05\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.2044e-05\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 8.1686e-05\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.1329e-05\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.0976e-05\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.0622e-05\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 8.0271e-05\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.9922e-05\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.9573e-05\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.9226e-05\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.8880e-05\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.8536e-05\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.8193e-05\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.7853e-05\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.7513e-05\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.7176e-05\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.6839e-05\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.6503e-05\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.6171e-05\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 7.5839e-05\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.5507e-05\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.5179e-05\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.4850e-05\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.4524e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa6ec37eda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04emayl05Yvq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2eeaafa4-5994-4bfe-b3bf-36715cf0275a"
      },
      "source": [
        "print(model.predict([4.0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.9858453]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HATgHDAW-315",
        "colab_type": "text"
      },
      "source": [
        "Loading Fashion Mnist data and using Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA-W1NkQ5hbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "a364cf23-0dec-436a-a666-2d91cd68b6f0"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_end(self,epochs,logs={}):\n",
        "    if(logs.get('loss')<0.5):\n",
        "      print(\"\\n Reached 50% of accuracy so cancelling the training!\")\n",
        "      self.model.stop_training = True\n",
        "callback =MyCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images,training_labels),(testing_images,testing_labels)=mnist.load_data()\n",
        "training_images = training_images/255.0\n",
        "testing_images = testing_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy')\n",
        "model.fit(trainig_images,training_labels,epochs=10,callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 3.8835\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5531\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5182\n",
            "Epoch 4/10\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.4957\n",
            " Reached 50% of accuracy so cancelling the training!\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa6de480048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVczL4AKTYtC",
        "colab_type": "text"
      },
      "source": [
        "Mnist Fashion data with Convolution and Pooling with Reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHSmgGdAThIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "1b6bd389-c8ec-4e02-fd94-16b39253ca25"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epochs,logs={}):\n",
        "    if(logs.get('accuracy')>0.90):\n",
        "      print(\"/n Reached 90.0 of accuracy so cancelling the training\")\n",
        "      self.model.stop_training=True\n",
        "callback=MyCallback()\n",
        "mnist=tf.keras.datasets.fashion_mnist\n",
        "(training_images,training_labels),(testing_images,testing_labels)=mnist.load_data()\n",
        "training_images =training_images.reshape(60000,28,28,1)\n",
        "training_images=training_images/255.0\n",
        "testing_images =testing_images.reshape(10000,28,28,1)\n",
        "testing_images=testing_images/255.0\n",
        "model=tf.keras.models.Sequential([\n",
        "                                  tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)),\n",
        "                                  tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                  tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "                                  tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                  tf.keras.layers.Flatten(),\n",
        "                                  tf.keras.layers.Dense(128,activation='relu'),\n",
        "                                  tf.keras.layers.Dense(10,activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(training_images,training_labels,epochs=10,callbacks=[callback])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.4520 - accuracy: 0.8349\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.3018 - accuracy: 0.8898\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.9045/n Reached 90.0 of accuracy so cancelling the training\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2566 - accuracy: 0.9045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa6daffb1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX52DNGRfTew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "92bb2e75-780d-447f-d089-546a23099d28"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 225,034\n",
            "Trainable params: 225,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNpMKjVAXv1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "c57048f9-13a7-48ee-e479-00eb0bfc6bfa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=7\n",
        "THIRD_IMAGE=26\n",
        "CONVOLUTION_NUMBER = 1\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(testing_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(testing_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(testing_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD7CAYAAAC2a1UBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy9eZhkR3Xg+ztxb2619L5LLbWWlkASAskyixE8AcaADcZ8z5ZZbDN++GEP9jz47GcjPDNmxm/8LHtm+OAzGMyADDx2ht0jMGIRAhu0IqG91WpavVZXdXUtWbndJc77I7NaWXWzqjKzcquu+H2fVJUn4944GZ11Iu6JE+eIquJwOByOwcL0WwGHw+FwJHHG2eFwOAYQZ5wdDodjAHHG2eFwOAYQZ5wdDodjAHHG2eFwOAaQVRlnEXmliDwuIgdF5KZOKeVwOBzrnbaNs4h4wAeAVwFXAG8QkSs6pZjDTX4Ox3rGX8W1zwUOquohABH5LPBa4JGlLkhLTofM6Cq6XNvM2InTqrq9mbZ1k9/LgWPA3SLyNVVtOL5ubJsfW6hOfMD7AA/4iKrevEL7dX1aS1WlW/de72MLNPzursY4nwccrXt9DHjechcMmVFePHTjKrpc23x97gNPtdC8pcnPjW3zY9vqxPc03mpUXMPEPehjvY4tQNzwu9v1DUEReauI3CMi9wRa6nZ35xKNJr/z+qTLucbZiU9VA2B+4nM4BobVGOfjwN661+fXZAtQ1Q+r6nWqel1acqvozrEYN/G1TVMTX/349kyzcwC3V9IZVmOc7wb2i8hFIpIGXg98rTNqOWhi8nMTX3epH99+67JWcIECnaNt46yqEfBHwD8DjwKfV9WHO6WYw01+XaSppz5HWziXUYdYzYYgqnorcGuHdHHUoaqRiMxPfh5wi5v8OsbZiY+qUX498Mb+qnTO0HKggKMxqzLOju7iJr/u4Ca+/iMibwXe2m89BhlnnB3rEjfxdY2mAwWAD4OLc14Kl1vD4XB0ErdX0iHcytnRdYwkD5c1Om7WaPmU85ItN6UbXD3Xul6OzuNcRp3DGecO0Mj4QNUAuec1x3rDuYw6gzPODVjK2M5jVRe0mf/NCAiCEUgbwRcIVan04vSrY93w4txbGsrvKH20x5o4uokzziswb3iXWgELC42yATwDQz54AhILVhVX5Nzh6C+vHf23LbW/fnvrq6o/PfThlq9ZinVhnJfyec6vcD0RUgZSBmKFYlQ1pqEqcQOjaqRqcG3ttT/vwKi1TVO9X9YoWU+IVbAKzHTpAzocjnOOc944NzLMXm2lm/WETWnIGGVj2jKaismHHqfKHpUY5kIIrcWIJFbQFghs1TzHtT5iVUKNGfZS5DzDprQlJUras9WwmFPd/rS9Z/H4XjqaHO93v/HLCdl//vTrErKD+eRMeN5Qss83PuPxhOxD311OS4dj7XHOG+dGWAUj1VVxrEKkgi9K1lhIQWSFshUiK1Ts08am3nSoKrYmSSGICBUbUyBAYiG0GWKFIU8Z9aMef0KHw7HWOeeN87z7oT6gO1YlUkWBsUpMSjxGUx4XpgPOT1fYs3EKVeFfjl3IIzNpSjHMhhZb5ziOVAnV4iFsynjkPOHxQsBj+mNy3mZG5q6lEHlcuSnmwg0zeGIXq+ZwLMtSPtKlfKF3HOqmNo5ec+4b57qf8wY6VKWiMRVCps0MvvoEdidZL2L7SJ6LLn8SgMNntnG0mAKE2XDhPRSIsRgMWU/YmFJiYubKP6PkneF0+pmkg2EANg/NYYwzzg6Ho3nOWeM8bwq1ttoVqW78AcxqmTHvBKFWKOssguGBqa2U4y1cVcmy76LD5Dblef4VD3NVfoTJmU0cmNxOYKum2apQiDymw2xts6/qIjE10532R3n26AiXbQjYnqlwem6UFaLzHA6HYwHnrHHWOheEpXpUKWOqFnLCTHCkcAeoRVEE4Xb/JHfObOFV5Rt4+dwwI7sm2fn7eTZc8woufvCrPPvWu4jmcqgKaoXS9AbOTG4mXxzmnrE9HC+mmd82HPF38LJdU1xz8UGOjO3miektdLEEW89otLk64i/MAPDfT3wg0eY/hucnZH9zLJmorPSuP03I/upTv56Q7dw1vqyeDse5wDlrnOFpN8S8+YgUFCWQEtVUs1UUiOI8RWJOEXBsbBciynmP3E9m6JuYo08RF7cRBym0tkEYBikqlQzlIM104DMVCHNSrUYiGNJeTDoTkPYjDGBdbheHw9EC55xxro9Bnj/JJyLEqkyGFSpEFGUqcZ3VMhpVuNP7Nn983/WMsosLv/d8tmeVDalfYFe2QspY0iYmZSxjpSEOF9JMB8JdxXFOcJC58BRaK4YZWkMYpsj4IXuG82jDbBIOh8PRmBWNs4jcArwaGFfVq2qyLcDngH3AYeBGVU1avAHBUN0ELBBQljKhLTZoFaNAsXKYezgMCH6whbQ/ykXez/HC4V2MppQhT0l7ysmSx8F8yJSWeCL+EeXg2Nk7KZZYDTY2+H7MSKbSmw/qOKe4ZKTxhP7nR5Jx445zj2ZWzh8D3g98ok52E/AdVb25VsDxJuCdnVdv5TwX9VjVs6vmuBaH7COkjFCxMOYdp2AnKYVnmribEtsCQWQ5bh7nnrkMGVJk8PHEMKNzTHhjlHSWsJJfcGWsIZPlLKenthDGHrFtLzOriBwG8lRr00eulp3DsX5Y0Tir6h0ism+R+LXADbXfPw7cTheMcyuGeb59NY+FEqklRskaj6wn5COYCA5QDo7TbK441TJRXGa6NM1PeKwqlJqhrW0mVlkYdxraImPlNMemN5PxI7KrO4TyElU93c6FrY5fPY2u3J5NTjLFJj7apvcdSwrf9+YmNfn7hOR3pq9v8lqHY+3Srs95p6qerP0+BuxcqmF9OZqcjLTUyeLsb820nw+hS4nBp3r8+kygTGuJ2FZoL4mnokTzvzZ3hYJFMKL4JsYVe3A4+stSbqKlaMd9lPK3t3xNGI01lK96Q1BVdbkyM/XlaDZ5O5ZsN2+E7aL0bYnX8+0bXDN/8s8XYXPKxzPwWHmax+M7CeMCUdQbt7iIh2+UtBcznK6wabiAtHdCUIFv1cb3H2pjWddP+xOfw+EYbNo1zqdEZLeqnhSR3UDXA0/rTZuqstSpjnlTnvaqmeFKpkShdIh+pb33jCWdCtpdOV+vqsdFZAdwm4g8pqp3zL/Z7MTncDjWHu0a568BbwZurv386moVsYsOjcDTq+PFq2cRORsqB7XNP1UyxjDsewAUIltN/ykLN+t6QWiLHCl4ZL0NXGUi9mwfx3itr5xV9Xjt57iIfBl4LnDH8lc5mmGQNlufl/udhvIPnP5KQ3kYTTSUyxJ/zmddcnUs9fi91L2bRUT2Ug0e2El1RfRhVX3fqm66TmkmlO4zVDf/tonIMeDdVI3y50XkLcBTwI3dUG5Bqs66Y9jzr+cNdFjbnBsSjw0pIbJwohwxpxUKpvcRfmFc4HAxQEmzdyhLdqiMMa0l7haRYcCoar72+y8Bf7lkezh7PH2eRkvpRs8bpoHwsg1J4ZkGEYGfmkqeCOw2m7Y1E23TFG1vtjqWJAL+RFXvE5FR4F4RuU1VH+m3YmuNZqI13rDEWy9bTcf1K+LFWePmWWpDsHriTqD2fk68s7kzAgvluLoBOGtmCeLeV/5UjZhkjlRpA9NBhjBI4fstR07sBL5cm4x84NOq+s1O6+pwdJJaoMDJ2u95EXkUOA9wxrlFen5CsN7Yzhtos+i9endFWPe+iODPr6ZrKT9TxrAtYxjyYbKinCgHzGiJA3o3xfI4sS3Qa39zFM/yUOVbPO4Nc9nsa7g+P0KqxXA6VT0EPLs7GjpYYbMVFm64OlqnFoJ7DXBng/fc2K7AwBzfbrRCPutXrq2KDcnHd08g40HOqxrgOa2QN3mK5dNEcb8OLcZE8SRRPM1MAGGY6pMejmVYdrMVFm64LheR5EgiIiPAF4F3qOrs4vfd2K5Mz41zvSsDnnZn1Mvr/ckAKRGGPINvhNEUZD2ophdSQpXqirmoHIvyHJIHqURztRVz/5kJLcemtpL2ulsNxTfC1sxC59BzNiedxNuy5YTsWCFZC2rIT/rIbz42GNWdv35XMqMdPNjSPdxma/cQkRRVw/wpVf1Sv/VZq/TeOLMonWdtRTyP1lUpmV8feyLkfCHrwY6sZdSP8YySEiUfeRyaEw7bCU7KQc4UH2Lxib3+YZmJQo4XRki5ZPsDQ6ubrd3mztInVm7UBKPZSxrKZ8vJmouq3VksSHWT5KPAo6r6nq50sk7ovc8ZMMYkql8bSW4KVlfP1arYWa/6M7BCPvIox0IpFgoRnIynmDLjlKMZFkZE9x+tre61zfwajq7gNlu7xwuB3wYeFJH7a7I/V9Vb+6jTmqQvG4JZU618nfFgU1rJGEvaq66EU0bJeTFGlIo1xCoEsSEfGUIrnKlUjfLPSiUe5McE8RxhnMfaANV2j2d3jwoRhShLyrnVBga32do9VPWHNI7YdLRIT41zdTMPPAO+qa6EM6aagjNrLGnPkjGWYT9CRPEin3LsEYqiKsQKhQjmIsuEmWS2eHBB0vxBJCSmFBkC44yzw9FPvpR/sqX2L0m/ruU+vlv5QsvXLEVPjbNvhM1pU608YqEETFQMRmDYN+Q8xYiSkhShCo/NGI5VylSoUJQikUQUJU+gRfLRGKphL9VvA+WA3k1l8mqkYSR35/Ck+hRSzxP5TKLdY7NJ2QdO9f4gyWr4/cf+sd8qOBxdp6fG2RPYmFLmIiEEohgKYdWgVFLCsC9nnRLFCH4cHeSp8vdBo4ZHUNcCs+XHeZDkhozD4XAsR0+NczlWnshbytZS0qiWiLMaWTEUpMkZ7+x2XsXGzMpYzTA7l4Bj/bFv5BUN5ZfZCxvKA20cpfR9ko/z/TsD4GiWnhrnGTvFN0pfACyqC6MqRJKP/bEtrNkVs8PhcKyGHkdrRETxZG+7dDgcjjXIwBzfdqyOU+E4//1EsqSTw+FYm7iTEQ6HwzGAOOPscDgcA0gzyfYbVjYQkS3A54B9wGHgRlV1W8COc5KrNg7z5RuShworlWTcOMAffP/ShKxMawem7pn7ZEP54SXa/8GOP2wo/3YhmbDqz/cmsngC8LfHnWtsUGhm5Txf2eAK4PnAH4rIFcBNwHdUdT/wndprh8PhcHSAZiqhLFXZ4LVUy1cBfBy4HXhnV7Q8hxGRW4BXA+OqelVN1vJTyUaznRcPLawW9vW5D3VB4+XZOnRNQvYLJpnisx+6ORxriZZ8zosqG+ysGW6AMapuD0frfAx45SKZeypxONY5TYfSLa5sIHXVSFRVl6pmUF+OxkiaTbmrqMRzhHEBqxWieJr+ZZLzMGYIIz5pfyNpM0JVT49YQ+Yqx4ntTFc1UNU7apNePe6pxLFuWMqfvxy/+60rW+6nHLfm8794tPUKRrfOvbfla3zzW43lzVy8RGWDUyKyW1VPishuYLzRtfXlaDZ6O/Q6nse4l+d06iRFO8VUqdC3zHIiKYbTu0ibEXZxMTt0M0YEDyEg5ieZHzFT6q5xXoKmnkrqJ76cjPRItfXJo7OW676VT8itNq4Eni/3vqjKh8YbJ7Ca3Jxc/Hxhxm38DTrNRGssVdnga8CbgZtrP7+64r2AjDHk4jRDOoqVmJS3hahPJaV8M0zG20BWRhmKh8gYD0O18opRIcsG8ma0Y/1ZO93yNcs9ldRPfJu8HS4BicNxDtHMyrlhZQOqRvnzIvIW4CngxiWuTzAqGbJ2JyHb2ZO6ENun6iUGQ8am8DBkxTtbRBaqBnp/fAXnZS7uWH/3lf6/Zps29VSyEq8Z+YN2LusJndbt63NrK+2pw7ESzURrLFfZ4GXtdJoxhsxZM5hu5xZdxwCbvTSbO6jffc03bfmpxOEYJETEA+4Bjqvqq/utz1rEnRDsMyLyGeBHwOUicqz2JHIz8HIReQL4xdprh2Mt8Xbg0X4rsZZxiY/6jKq+YYm32noqcTj6jYicD/wK8FfAH/dZnTWLM86Oc5ZOHfABiLXETOmR7inbRfoQmfFe4M+AJXfT6yON9uQG07XZb5xbw3Eu8zHcAZ+eIiLzk+G9y7VT1Q+r6nWqet2WdOvxxOsBZ5wd5yyqegewOBD5tVQP9lD7+Ws9Verc54XAr4rIYeCzwEtFpHEGJ8eyOOPsWG80nXZARN4qIveIyD29UW3to6rvUtXzVXUf8Hrgu6ra+AicY1mcz9mxblnugE/t/bOHfJZr53B0A7dydqw3TtUO9rCaAz6OlVHV212Mc/uIau8WBCIyARSA0z3rtDtso73PcKGqbu+0MnB2bJ+qvWxXv0Gi1c/QcGxrSaX+qS5a478Ck6p6s4jcBGxR1T9b6eZ143sujG2zzH/Wrn1vIfHdbdR/v+hV/42/u700zgAico+qXtfTTjvMoH+GQdevGTrxGWoHfG6g+kd2Cng38BXg88AF1NIOqC6RvahLeq0V+v1Z13v/zufsOGdxB3wcaxnnc3Y4HI4BpB/G+cN96LPTDPpnGHT9mmFQP8Og6tUN+v1Z13X/Pfc5OxwOh2NlnFvD4XA4BhBnnB0Oh2MA6alxFpFXisjjInKwFmM68IjIXhH5nog8IiIPi8jba/ItInKbiDxR+7l5AHRdc+ML1exxIjIuIg/Vydz49oh+j/9K4yoiGRH5XO39OxsURF5N3w3/vhe1uUFEZkTk/tp/f9Gp/pdFVXvyH+ABTwIXUy1/8gBwRa/6X4Xeu4Fra7+PAgeAK4C/BW6qyW8C/qbPeq7J8a3p/mLgWuChOpkb33Uw/s2MK/A24EO1318PfK6D/Tf8+17U5gaqB5l6+u/Sy5Xzc4GDqnpIq+W2P0s1Q9hAo6onVfW+2u95qtUdzmPwsputyfGFNZM9bs2O70r0efybGdd6Xf4n8LJa4elVs8zfd99ZlXFu8THvPOBo3etjDMggNEvtceoa4E5ayG7WI9b8+C7CjW9/6dX4NzOuZ9uoagTMAFs7rciiv+/FvEBEHhCRb4jIlZ3uuxFtG+daAccPAK+i+pj/BhG5olOKDRoiMgJ8EXiHqs7Wv6fVZ5+OxySeqz7OVunW+DqaYz2M/3J/31RrM1+oqs8G/o5qCoDu61TzqbR+ocgLgP+kqq+ovX4XgKr+9VLt02T/dcgsWblmSeq/GfPPMr4IKVOVhxasKiUCKnaWZr9HnuTYaEbJeMqurVOYPXvRuZOUT6YII59y7BNYwWq1j/r7Lu5BFskbPXNN24nT2mQCmdrkdwB4OdXVxN3AG1S1Ya2kjOS0mbE1DZ4GN6WjhKwcewnZiWBixfvvSO1IyPZePZyQ2VLyXscPDCVkgW3u37KVsYXqxAe8j6rP8yOqumwRXSMp9UwmIY9sodku1zoHVPXyTt9URF6wJWv+9YKNye/bctx/Kuy0Kh0hbTa2fE1gZxp+d1eTW6PR48jzFjeqrxXmic8Nw7/ZckcKRLVJxBdBgC0Zw46sEiucKApzkeVBe5hDhW8BcVP3Hc5cxKtyv8jFI5Y//c2vwH/6PXLf/wCPvO+ZnJzaypMzmzhRSpMPDadKSqSgNfNrFeKaTqam07yu0Ng4fyX//kaZt5birC8OQETmfXENjfOQGW1qbIf95MPS6y5I5v15fGZDQvbvD//Divd/47bXJ2T/7e7nJmTBw/8jIbvp5c9OyI4U7Ip9QmtjW/fUd3biE5GvLTXxAXgmw5bc1Qn5eOGuZrtdw8QAX+3Sze++YKPH99+0q6WLNr7nRJfUWR3n517U8jWHCv/U8Lvb9cRHWpewfLO3o6VluieCL5D1hI1p8EXJ+UrKKPMGOB8aDpXneMocYjo6CjT3xwwQ2hLjlRDF54Nfeg37v/0IIi8BIGViXnTRQbZsneLosfO449he5kKPM4GhEFVX0qXYsnhh15FdiipNTX6Otmhp4nMAsOyTRbuoanTNLlfgtRGrMc7Hgb11r8+vyTqGL+CbqmHeNxKQMZYNqZCsFzETpjldzjCrcMg8zqnCj1u+f2wDJshTLGf55th9TB16mEuGX8Hv7zyPSzbMcsnzfop53WVs++fbmfniLzNRHObQ3BCTFY9SDJEVrFRX0E0+fXeU+qeSnIz0XoG1S8tPfUbWtwHR1tKqtuQycjRmNcb5bmC/iFxE1Si/HnjjahUyAikRpLZiThkox/BkPo0RGPYzpEQpRMJ0KEwHllK8YmX7hlitMGUmKGqOUjiFEjHNGI/N7qMQb+Lyu6/iIvNT5o6fx7aRWXKpABFlWybNmSDFmPEILOTDp32jHTTSK05+q3kqcaxM/fimvBE3vk3QjsvI0Zi2jbOqRiLyR8A/U50hb1HVh1erUNoIG1JVozzkK2mjHJgVvhv+iFI8hSfVMupWLVZDYhsQxk1P6guI4mmOFn4EYlBbAmCy+DCfDifIFTdz9w9ewP77r+a5W2d4xYt/SGq4xDMmNxEUsxw/sZsHx3czG6Z4ci7FmQrEWt2Y7BCrnvyG/KSTZf+GpD/+1x/4THsaNuC9Jz+QlJmkrPLeVEL23/79jxKyz3zoTQnZl48kNxhbpOtPfesY5zLqEKvyOavqrcCtHdLl6fvW/rMKqkIxtkyXDxHbmY73ZLWwIPRCtUwpOEI5OMFDI9vJT1/IjuwwWIOXCcmMFvBSERvzebbObsITJWt8UkbAKp3aQ+7W5OcA2pj4rtoe8P03HUvIX/vBtzRsf3vpo6vXskNcPPyqhOxQ4Rvd6q5ll9He0dYiNdYLA1MJxRPBCEyHMQ+E41QokyKNrz4TchTb45AlJWas8gj51ARzY1dy5EuvYlNauXS0yI5cke0jea697HGCcgbvycs4UhhivOxxvNg5/3O3Jr/1jpv4+k+9y+iaXWnnMmrAwBhnI5AykLcBByrf7cIquVWUSniCSniCMzzAfSXwvc28Yvo3uXLjEL/oRzzn2QfQ0OfSqS3ANkI7zImicI7H658TuImvaziXUYcYGONcji3FGPJSRJsIhxN8jBlGsVhbpNnY5tVgbcBRO4U/u4XR1A423/ZCfD8miHx2DhUYL2Ux4hM72+xYv3QlUGA9MhDGWYGpOGBG8pyWE6hWVrzG8zayKbsPqxHT5UNYm++6nlaLPFS+lUckzQ/jC/ji+DPYazbyB5ef5pn7DnOmlMNMJg9v9ItilJwlThSb8+818lNO2mSs/Exp4T7PW7b9YaJNucG8mXlHcpPwc1cnN/9efs29CdmXj7w4eUPHQOBcRp1jIIyzVaVCSN7MEtjiWbngg/igEcrCI8YihjRDxBIiDVOELD6z1whBJMWCFCMN+qp7E2vzWGA2MJzKDJGLLyWwAzGMDsdA4FxGnWEgrIoFJrxxTlYeJLYlVEMEn/NGrmdvfCEnvOMcKdxBNaNgFcGrhtVp1VAvxMP3NmAkQxTnqxEZDUj527gs/WJGdJgYi8Uy4Y1xrPCvqJaX1XlP9tm8auhyNqSUn+XTjD/8LI4UstXeRc4e7XacI+zaBO/8lYT4+Z9PhgQC3J4M7OgbXYzMcHSRgTDOAEWdohLWnZcXn73xhTxrZITU3IUclcxC4ywGg0El+ZguCClvlLQ3QkFDbNzYOKe9Dez3drAlY4gtRAq5coYTci/xCsZ5R7yb67YW8EV5bHaYiXKaoJYcycjToYAOh2MFlpj4luU9ydwsg0AnJ8KBMc4JNOKEd5zU3IUc8Y4l/NBRXGA6PoFqjLXBwktRwriA1QirC9+rJ4hnORSfYaIwhMUSYRn3TmKb8HnHxJSiDFkvZms6Ytg3TAeGyYoQ2nk/q7PODoejPQbWOCsRRwp31FbMlQWrZoDYzjJTmpt/tejqmCg+Q7RCAEcYnebB+KsIT6++lTDRVyMqUmE2HMWIsm9kjqFUwNG5DahmKVshKjOQURsfmfxEU+1KLE5pm9z8a8RHTyc3+prlWCF58u/o0fPbvp/DsZYZWOMMoBo0ZSiXuLqpNqrltta3oQQUIo+sp/ieZSgdkDYWz4BY6EwRHYfDsV4ZaONcjdYwoDYRQWHMCLnUDqyGlMPxFTfwOs0ZTvDIzPnsyKbYM+ST9iPSXkzaKKGpnnZ0nDuYiUnSH/l0Qn7zsXWTbN/RYwbaOCMGkQxKJbEQNpJm2N9KrCGVaKrnxrkSz3LSFqA8TKyCb2JSxmIATwbQn+FwONYUA22cVWOgUvu5kLQ/yi69iFAC8uYkQd0hFJE0I5kLSZsR8sFJgmis47qVo2mOZJ4kjC7kdHmYHaVh8mGKUIXQSiez0zkcjnXIQBtniBsaZoARfwf7va0E1nLMH11ggD0zzDPk+WyXIX6afZxjc6fodOREGJ1mLJpiNjPGkcIr2JwZYaqSJoirIXmDQKOUoc2epPylzHMSsjkvWUbqi7N/37piS/AnT344Ibsl/W86dn+HYy3RdvXtTtKOErGGlOKYoo2xmszFUZGAilpi7VYhSEWJiLVCxQpB7BGrYGsnE93C2eFwrIa+rpznq56ECl7c+KTVUkyVD/KD9BxWLcVgYdKrKJ7l8fAHPGlyVKIzdDPeWNVSjqEYe5RtNcY5bL6MocPhcDRkReMsIrcArwbGVfWqmmwL8DlgH3AYuFFV26oVJQJeG2VRrc2TLy/1iB5X0322o1AbxAqRNcRWzp4MdPb53EJjj3AqmdTqlcO/1bD9NwsrVyp3OJajGY/Cx4BXLpLdBHxHVfcD36m9bpmKVSaCkJNBmVItf7NnNrJj+LnsGX4Ro9n9pP1deGYjHa1r3UFEDEM+DPsRu3MBz9xQYf9oxHADf2/r95bDIvKgiNwvIvd0QF2Hw7FGWHHlrKp3iMi+ReLXAjfUfv84cDvwzlY7D6zlqIxRNHlKlWodwKH0Dq7jGrKe4aDu4VTmKeaiCQqVOXqRs7k1BE8yDPvKxnTAaLrCplyRM8Vhjpe2M93u+ZmFvERVT7dz4R1h8kTfmzYnU3p+aip5qu/zc19PyErBkRX7vHQ4mSPhYOF/rXhdlWSelCt3nEzIvnbs4ibv53CsXdr1Oe9U1fm/mjFg51IN62uF5WRkwXuRWkqmQMnOENuqEyLWkDkbEapHWYpEWsE23NQTnl74t2u0n04Zqhq2fR9PIGVifGPxjMUT59RwOJpF4hBTGO4rtUkAACAASURBVO+3Gh3BmNGWr7F2uqF81RuCqqoiS5+6qK8VttnbsaBdQMzp4CCVcBJbO0RSCk5wV+qfEUw1eZEtUy2butBwiqTwvY2oWuJ4ZpkczEtjZIhNuUsx4jNTOUIYTbR4h+rHyXmWDZkKnliCyCewfqcy0inwrdr4/kNtLM+y3MTncDjWNu0a51MisltVT4rIbqCtaS8iJlyUb1k1oBysnAxXSOGbHIoltgXQmIVRGSsn2xfxGTKbSWuOgpkgbCpBfxJf9OxqOYq9sxuDHeB6VT0uIjuA20TkMVW9Y/7N5SY+h8OxtmnXOH8NeDNwc+3nVzumUZPk0rvY7z0XXz0KqQIhAePxQWbLBzBmhH2569lit3JIHuRM8ac0MriZ1Gau1svZlPY4pecz5RUY905wvPDjpo+DR7bEoTmfkdQ2SpFHITKUrTAdrN5Wqurx2s9xEfky8FzgjuWvcjSDiBwG8lQfySJVvW7ZC7YPI297bkJ83wd/0g31OkqjR+1ulXUTkb3AJ6i6OhX4sKq+ryudneM0E0r3Gaqbf9tE5BjwbqpG+fMi8hbgKeDGbirZiI3+Hp6V2ULWg1g3YhXuK43wIE+S8TfzwszF7Bu2fPv0c/gRD9HInzzi7+CqTR57ciEzoUch2sSB2Y2MmQeJ4uaMc2xLHCpUUDJMVixjUQGDsNlkSZv2z/iIyDBgVDVf+/2XgL9s5R7H5m5PyD5FUtaIN296TUJ23lByDP/j4Q8teP32PbsTbcbK/zYh+6ujH2zQa/L+hUo2IfM6l/Kv7c1Wx5JEwJ+o6n0iMgrcKyK3qerK+WYdC2gmWuMNS7z1sg7r0hJlnWWiEpExBqvVet2zZhpFiW3ARCUiZXymzSxLRR1X7BwnS0Jg0xQjqMRwOiqj2or/2pAzHjkPNqcNwjBKRwL/dgJflqoh8oFPq+o3V39bh6N71AIFTtZ+z4vIo8B5gDPOLTLguTWWZqr0ON/zJxbUDwyj6lNqGJ3me/J1/Gj5E4Kz5UN8yVbwSxmshli1hHGB2M41bN8Iz2Q4b8jnGRsqZL2YrBeTD1PcP5VlsqIo2tbmoKoeApLJLBydYtnNVli44bp3T67H6q19aiG41wB3NnjPje0KrFnjrFpeWHOw/j2ipk4IqpYpVJ5clR4ihmEfNqQDRlMBGzNlZipZsrNZjAxmNRQHsMJmKyzccL32WZvdv2QLiMgI8EXgHaqaKKvjxnZl1qxxHhQEQ9ooQ1412T5Uj6RnPWXYF8oxlAfcQj9nKOm5migndf5JvrjivbZlk776Tx5r/zSOatJBdGGymhUNqmqtcF+32dotpHp44IvAp1T1S/3WZ63ijPMqETFkjJJLhfim6ts2KEO+Zdj3UJWBN87rjXY2W3WigH3/3Qn5eKHx09sgkU1tTciKla5FawjwUeBRVX1PVzpZJ6wx4ywYGcKYLGCxGiEYfG8YIz5BNENsZwDBMxswJk0U55cMizNmlC25y0iRZS4epxxNY21AbGdpNtY5isscKRhGpjazPROwezhPaKt+cFeqamBxm63d44XAbwMPisj9Ndmfq+qtfdRpTbKmjLPgsTl3GZtlD6FUKOksPhnOs/sYIcMT6YMcm/sBRrLsHrqGUd3M8fgRZssHaGRst+Qu43c2PI+tmZj7zng8Zk5xRsYYK97ddGHZMD7DV0vf57vBDm5IP5NfPi8irj2KewLeQGTMdtTjNlu7h6r+kEHNUrbGWFPGGTH4kiGnQxgMsUT4ZBgmzYjnk9bc2XZpzZGzOXyTjJOdJ0WWbdmYHdkKm9LD5MIcvsnQSvp/1ZB85ShFc5pJ3U9gDaqCrRnoDh3jdjjOYRSxg5bUrD1ekPn1lq/5l9JHGsrXlHFWDTlTeZI5bwKrIbENMMZn1h/D1wyz4QnAorbEifBBJkyOUnCapVwURTvFozM+J0s+DxXyHNC7qVTytSRITWuFtQWsVpi0JY4VcliE8bKhEEHQR3+zkeTOWf1R+XnuL34mKWuzz0/+LJnj487Shxq0bI5sOvkEsyHtEks5zn3WlHEGJYwmEgmKFufiUCLKwTFWOuNXiWf5WanESCXFk+ZhZguPt6lVBBoxbWY4Va4elZ0OlFLNMLtnPIfD0SoDZJznU4BaullWqp4oLjGWOkVGs5Tixmn7VkLwGc1ewoi3g8tkN3tyEbFCYH1SkRDESsX5NtY8sn0I723XJt94z+BHa1xjXpKQ/Qv/2AdNHK0wMMZZ8BDJoIRNb8atlig+w6Hi7QCotlfUyvc385LUS7l0VNg3HLB/0xSVyMc3G5kJfCYq0jBm2OFwOJZjYIxzf9CGPthW8UXIekrOjxhKV/CMZcizlD1LyngYcRuDDoejNQbGOCtxLR55bW32RPEMP4jv4cEzO3l1tJcrt8cYUTakg2oyprB/sXSrmXj+4Rm/m5B9+2QyB8IXZv5+weuP/O/fT7TZ8/Hk16zZ4gijQ8nPcMFQcxkDHY61zMAY56qfee2F06gGjBfuYhx4ynsbRiy+HzPkRYS+IWXS/VbR4XCsQQbIOLdGJrWH89LVcwRn7FECO0cQzRDFUwg+ucz5pM0IhfDUkuWnMqk9XOu/nCFJcz/3MFmcT5zeXkWUibDCg+O7McDJUoZC5HGmIi3fx+FwOJpJtt+wsoGIbAE+B+wDDgM3qupU91RdyMXp5/EbW3cB8JMzF3KSOQ5nH+V04T48b5QrzPXsMMP8NPs4x+a+TyMDuS/98/zFM+fYsWGG//eeF/BF5o2zQRCUxaWvlucx8yBfPnotBpiLI2IisuKR8zwXTrfGsafKVN7zUL/VaIvj3uBHlDiSNOMQna9scAXwfOAPReQK4CbgO6q6H/hO7XXPsDXftGrVS23b8FUbDJ5RfD/CX4X1FHxE0nikiFWr/6HE2BVNu4jcIiLjIvJQnWyLiNwmIk/Ufm5uXzuHw7EWaaYSylKVDV5LtXwVwMeB24F3dkXLBhwO7uYTZ54FwHR8nEo8S1CZAZQ4zvNwfAdPMEIhOMVSq98T9nE+dfAlbEzv5ifRkbp35o3qyqtmwWdj7hmMeju4Vi7nF7bHxCocL2YpRNV0oStkpfsY8H6qTyfzzE98N4vITbXXPRtbgN9/rL042F/9fLIUn/Jo23qMTSUzqh3ID7V9P4djrdCSz3lRZYOdNcMNMEbV7dEzKuEJDi+TbL8UHKG0wj0KwSl+bI6RKw9x0j624A5NIz4bvV3siHdzwUbD/g1ThNZDdYSZ0GOysnzKUFW9ozau9fR14nM4HP2naeO8uLKB1BXZVFWtlftpdN3ZcjQ5SeZd6CfWlpnQw/iSoRy1d0IQLHk7AR4Uog2IVPM5zw9Pm2lD+zrxORy9xGS2kL34N1q86p+6ostquXy49ae6f1liFdmUcV6issEpEdmtqidFZDcw3uja+nI0m70dAxW2YLXAmeK8q7e9+GrVkJnyYebMKabNfgyKZ2wLee1Wuv/anPjONfwL9rDx/f85+cbfv7n3yrTIS7OXJmT/WPhew7a9Op3rWJkVbcgylQ2+Bsx/M98MfLXz6gngIZLGyHAty5pX0ytL2t9Fyt+OtB0R6GEkizFDSO2+C3qXLJ7ZiMjSaUcBrFaIbYnQKnEtXag9+15bip2qTXisNPGp6nWqel1GXJFMh+Ncohmr1rCyAXAz8HkReQvwFHBjZ1UTfG8TKW+UnL+ZbXIBERHHKj8hiMbYNfRzvDT9HMqx8u3wdmZKrVde971NXJK9npwO8aS9h3z5ifre2T308+y1F3HCO8axwg+XWFUoqhWsRszFEbNBBoDACpFtO8J5fuK7mQ5PfBcPvyohu0jPT8hesyd57TsO/o8V7/+yraMJWflM8pH1GantCdni04YAJwvJJ4Jy7AITBx0R8YB7gOOq+up+67MWaSZaY7nKBi/rrDoL8UyOjLeBUbOdHfF2YmLGvCxBBFt0F1dsjCjGhh+d2cpMG/dPecPssdsZ8nxOyGYWVFUTw1bdxQXpEcJgF8dJoSz1yKcoESGWSi3ZfmgFi6y4chaRz1Dd/NsmIseAd9P1ic/h6DpvBx4FNvRbkbXKQJ8Q9EyajBkh0BKHvUOEWiYIqmY4lIB85FGMhIj2MsoZSbHBT7EhZdhZvpBCZpIwLhBEE6CWSRnjSJBjwhtDWTkB/4zkOVLYBsB42VCMIFjBOqtqsvR1la5OfA5HtxCR84FfAf4K+OM+q7NmGWDjbEiZIXKykVk7xlTx8VqFkmr+jYqUmA2FYgRRu+k+TYbNacPWjHJBsJVIruaMf5Lx+AyqIVPhUwSpIsVwEtWV835MmQmezFcf16fDiIqNyRiPrHGFBB3rivcCfwYkfVyOphlY4ywIW82FXGL38pSkmeJx6hMjBVpiOlAqMcQtlZV6mshWmAosqoaZOKBiygsMfWRLVOJZoni5bHkeKX8LKW+YzXY7KfO0K0NdTo2+IiK3AK8GxlX1qpqsrbQDtjJF6fCXVmq2LFuGGteUzUrjJ/8ThR+sqr95bjndqExYd5KMicj8eN8rIjcs0+5spNEFFyQPGjkG2Dh73iivGrmYV5w3we0n9/J3lVHC6OlUkdPhUR7QXYQSUAwbJzZaiWJwnG/zLfwoUzXCQQFrg9oKXYmiKebiPGjEUlt7vreB61Ov5eLhDEM+jPpKOYZCZAjVdCykrlN846VJ7/zlX/9GQvboyRe1df+bjyU39RrxYJP3K8fJKJrDc02HPX6MATx9eY7zQuBXReSXgSywQUQ+qaq/Vd+oPsT2uusudquYBgyscRbx2ZOL2L/7OAdnN+JNZhZ4fcO4wOnUcWJCYttefl/VMoXKk0u/X6sNuBxGMpyfy3DJaFS7J4gYUkYwLqqgr7jTl71HVd8FvAugtnL+vxcbZkdzDKxxtjbgaNHn0eN7OTyXJrYL/cqxLTAdHEHVtl1iqhOIGNIGhrxq6iVVIVIlZYSUCJ44Az1gNH36sv7Re+8eF0fu6C0Da5xVIw4XIu6d3MzP5mKsDRa9X6ayRG6NXiJiyHiQ8yNUhViFSCFtPFLGuFShA8xypy9r75999L72WZvdo3eLqOrtVJ9MHG0waC7RBVhVtO603VrDGeaBpKnTlw5HvxHV3i0IRGQCKACne9Zpd9hGe5/hQlVNHo3rALWxfar2sl39BolWP0PDsa35nP+pLlrjvwKTdRuCW1T1z1a6ed34ngtj2yzzn7Vr31tIfHcb9d8vetV/4+9uL40zgIjco6rJpL9riEH/DIOuXzN04jPUn74ETlE9ffkV4PPABdROX6rqmV7qtVbo92dd7/0PrM/Z4Vgt7vSlYy0z0D5nh8PhWK/0wzh/uA99dppB/wyDrl8zDOpnGFS9ukG/P+u67r/nPmeHw+FwrIxzazgcDscA4oyzw+FwDCA9Nc4i8koReVxEDtZiTAceEdkrIt8TkUdE5GEReXtNvkVEbhORJ2o/Nw+ArmtufKGaPU5ExkXkoTqZG98e0e/xX2lcRSQjIp+rvX9ng3wpq+m74d/3ojY3iMiMiNxf++8vOtX/sqhqT/6jWvzvSeBiIA08AFzRq/5Xofdu4Nra76PAAeAK4G+Bm2rym4C/6bOea3J8a7q/GLgWeKhO5sZ3HYx/M+MKvA34UO331wOf62D/Df++F7W5gepBpp7+u/Ry5fxc4KCqHtJqMb7PUs0QNtCo6klVva/2e55q6Z3zqOr+8VqzjwO/1h8Nz7Imxxeq2eOAxQdB3Pj2iD6PfzPjWq/L/wReVis8vWqW+fvuO6syzi0+5p0HHK17fYwBGYRmqT1OXQPcSQvZzXrEmh/fRbjx7S+9Gv9mxvVsG1WNgBmg4xn6F/19L+YFIvKAiHxDRK7sdN+NaNs416rrfgB4FdXH/DeIyBWdUmzQEJER4IvAO1R1tv49rT77dDwm8Vz1cbZKN8bXjW3zdOv7PUgs9/cN3Ec1/8Wzgb+jmgKg+zrVfCqtXyjyAuA/qeoraq/fBaCqf71Ue4/Mv2YkWeq+FeaTcKaMIWNARPEFBGUmFGbsaZr9HvlmmO3+EFlj2bz9DLJjH1ocJxqPiUOfuSBLOTbEChVb8wWtovhUUSdPa5MJZGqT3wHg5VRXE3cDb1DVRxp+FslqM2PrS3I+3phKfqJGz4xHKivngLkwuy0h23ZlspScFk8lZOOHhhOyM0FzOQm7ObbVa4w2WsvsTDVewJ0Ku5fsTkg1lDdThHgVHFDVyzt9UxF5wdac/OsFm5IVb5bjJyeXL4KxRG8ttd5kkt/llZi27eRJ0obf3dXk1mj0OPK8xY3qE5Z74nNl9jUtdzRvkA1CCh8PYWcmzQXDkDHKhlRMyii3jXncWriFqutqZbbkrub/2PJzXLahwI1v/CLh23+PzE8+xZmPGKYmtnL3UxdzYHaYqcDwVCEmsJaSRoRUvxi2RTN9d+ljjTJvLcVZXxyAiMz74hoakIyMNDW2W81QQvYre5L15LwGaY7fduCjK97/P+xLuiZ/9+4bErLo3vcmZB94/c8nZJ89UVqxT+ju2FYxGJOcZN607Tcbtn7PyebKdbVDyt/RUB5EY13qMQb4apdufvcFmzx++JbWgkGG/0vTuarOIpJuqf0Nw43/bZfjK/l2DhUGDb+7XU98pHUJy4fNtqas2dnVMT4+wrCXYmfWwxNImer8NxUod52pENUZyCfNIzRTJXue6coRvnH6Uu6aHOFnH/5trv7CI2wduowLzz/Gpi1TvHTDA/xv1jA9tYmfnd7JXJDmsdlhxko55kJlPAiIUUIios5nnV5x8quf+NKSXHU6lqSphYVjATd346aqGl27p/GTwHpnNcb5OLC37vX5NVlLzDsJZNEjR3WVbEiJYVva49LRiIxRPFFElLtOZ7hPf0g5miGMzzBflLUVgmiM+6LPAHDbUwZ5Snjp0O/ywW0TbD7/FEPPmcDuv5zzfvYjLr5vM8XJjWy4/9kcmNnIyZLPXOwTWEuslqWrc3ePdiY+R/PUT37rvXSCtpZW9ZXA+6iGyX1EVbti2M91VmOc7wb2i8hFVI3y64E3tnszRRcYaItSJAAFUxG8vI8v4NfcfsfLZQJbILal2mq5Xds0f12MAifkNHcd2s8Fk9s4/6kxNmybIg4vJKqkCSppdozOkvYiRvMbqdgchcjjeAmi2oq9VVfHMnRk8nM0pKmxrZ/8RHw3+TVBXaDAWX++iHxtOX++ozFtG2dVjUTkj4B/pjpD3qKqD7dzL4vFYM4aaIMQYRn3TpK3E8Q2JMwXUbVIbUMriPLE8QzKagxzkgOl7/KOn11KmiFG2E1WL+Ga7DZes/cM24fnuPJZjzByyXHO/PQSzn/0CsaLw3zv1DClUoSiBLSzUdGQVU9+jbYuZxpUKv93T3w8IWuX//Oxf0zKTFL2hauTH+X/+utPJWSTf/zmhOy2yXyb2p2lowsLxwLa8Oc7GrEqn7Oq3grc2iFdEgRaohRNEcYFovgMvYjmie0Mpwv3AiD4IAZ4HdcWR/GMYlIR3raIoW3TbB+ZxaqQ84ZJYQixGKQjq+dOTn6OhbQztsOymedkkpud/89bPtuw/Xv+y+r1XGoDayjVOIrgl3OvayhvvEnV/N5Mi7QcKLB3g0vx04iBqIRiMFgsthYhEAIxEYX4NJVwshYi1PunSiUGVX5m7+OTx57PLm8Tp0sv5YoHTrFxeI69e4+xJT/N49ObCGyGqcAyFnVs5dz1yW8948a2v9S7jK7d0yCW0zEYxhnAimKxVP9vCSUgsAWsFvqolQIx+fIT3M0T+N5m4qM3ciC/jxt2jbPv2ocZKmY5/9B+poINxGoY65xtdjjWIm6vpEMMjHE2KiCGigTkZYpAi0RxczGuvcLagKN6Bm9mK7tzm7ny4AWoNWS8iJ25gOkwgyl1xq3hcKxRnD+/QwyEcZ7fEDQK06bIWOkBYluohccNDlaLPFz+Jo9KmvzxX2ND6tlsTAVszJa5OjdOIdrNT2e9bsQ8t8Xi8ESAH5U6t/m3Gn7jp59OyP7+P7wlIXvdpQcTstsm+51mw7EUbq+kcwyEcZ5HEJS4Fh7X3Cm/3qJYm8cCZ2SOqcpGADblSuRSAakGp+ocjvWG8+d3hoEwzgbTcJU3yMREBFYIYo+5IIOqUIyrn6RTERuOwSFtDHszuYT8a1/95SWu+GTT986k9rSkSzmebig/LI3lXYzMcHSRvhvntWaU57FiiVQIVSiF1WGM7Nr8LA5HPynMjnDnt1/U4lWtp/rIpBrnJFmKw0tMgssxnNnX8jWFyoGGchdguEpUhVgNUVwdSq+WnMms0UnH4XAMBn1fOa9llGpGjYo1gI8FcsbDqBBr3HfXRnqN/fMWomTayGMzW/qgicPRf9zKuU0sltgKsRVU51OagidS8zu7oXU4HO0zMEur+Y00T9dG+sCi5BkrG8pW2JAOyXoRWc+S9aqrv1Ls3BrnEtuzAW99RvIsxTePrr5SVSU80VCeTZ/fUO5JpqG8KHMN5cOZSxKyQuXJJrVz9IsBM85rZ7UZaJHZQEmZqhH2jMU3SspAaMX5nB0Ox6oYGONcIcSKEshgnQpcirKd5XhQJtIs8cbqKcGsZ8l5glUw7hi3w+FYBQNhnBWlZEqUpEjRTtGPxPWtUghOcSD7EHPRJdygw4xkKoz6EaOpDCB4gel7ScwOpi/tCX9z6t6E7Eu7dzdo2VpJI4djLTIwfoSQgIrOEdky6OAbZ9WIcjxDSYpYwKrgGUvWUzIezq3hcDhWxYrGWURuEZFxEXmoTrZFRG4TkSdqP1e1lLFYpvQE46WHmKscqyXQH2xiO8dc5Rin9QhzoUcY+2S9mPNyATuzMWkzMPOew+FYgzTj1vgY8H7gE3Wym4DvqOrNInJT7fU7V6NIYOeI7cxqbtFjYqwWKMVThFoNqfPEMpoKCazBl9ZKvTsGm5FdeV70Z7cn5O9602ubvsem3FUN5dOlhxrKo7hxulzxGk/8BwpfbygfauPUmqP/rLi8U9U7gMXFHV8LzKc3+ziQLBGxjoht9ZSgESVtYtLG4nXAqyEih0XkQRG5X0TuWf0dHQ7HWqHdDcGdqnqy9vsYsK5zOMZaTYAEkPUjslGM0LjEUBu8RFVPd+pmg8x8ebCFvDoh8Qdnq8Th6BqrjtZQVRVZOldmfa2wtAyvtruBQ9XWbQgqnlg8UYzbD3Q4mmJ4n+V5Hy+2dM2m5zR2ES3HXNBaQZbH+ZeW+5gvQN0J2r3TKRHZXVVGdgPjSzVU1Q+r6nWqep1Pts3uBpvQQhB7GJThVMBwKiRlqiu8VUZtKPAtEbm3NsktQETeKiL3iMg9Ecmq2g6HY+3SrnH+GjBfs/7NtJO/7xxCFWKtDmXKj0iZuFOpUK9X1WuBVwF/KCIvXtjvuT/xdQvnz+8OIrJXRL4nIo+IyMMi8vZ+67RWWdGtISKfAW4AtonIMeDdwM3A50XkLcBTwI3dVHKQsRqRj4TZMEXOD0l7ETk/ZDQFG4MsZRtRoL2qLqp6vPZzXES+DDwXuKOD6q93mvbna3aY4MrnJuR3lj7RoHVjKvFs85pRdZk1lLd4SKtYOdxS+1USAX+iqveJyChwr4jcpqqP9FKJc4EVjbOqvmGJt17WYV3WJKqWQqScCVJsyXhk0iHZMGQ0pYz6HjZUCm2cFBSRYcCoar72+y8Bf9lh9QeeTSP5hOz15+9KyH70RC+0caxELVDgZO33vIg8CpwHOOPcIm7be5VYDcmHlqmKYTZMUQlSRNYja5TRlJAxbfuddwI/FJEHgLuA/6Wq3+yo8uubZf35jtUjIvuAa4A7+6vJ2mQgcmusZawtc0BPMje9HcixPbsJq8K2bEjW87DqM97aRjQAqnoIeHan9XWc5XpVPS4iO4DbROSxWkz/WeojjfbucT79VhCREeCLwDtUNeHPcWO7Mmtk5SxUq6z3+tSdh+Av268Sk+cMp2WGmVAoRimC2CNjLCOpmPQaGeH1Rr0/H5j35y9uc3bDddvmjsWtn/OISIqqYf6Uqn6pURs3tivT95WzQYgBWWaeGM5czNXmegAesN/vyQaHSJqLhl7GpbqXw3KSJ4q3oZoMV1MNmQyeZM4b5+LKSynXDHP1lGA1v3O/qnE36075yDP/TUL2e49+rLPKLOLv9v9eQvbvnvhIQnbPkYsSsq2Zyqr6dv787iEiAnwUeFRV39NvfdYyfTfOwIpJ9renLuWlm4dQFU5MX8pTPTHOGX7Ov4gXbg+5a3Ivh8o5orhRLHFMJTxBJRROyy8QxB6+KCljSQEpt3IeRHYCX67aEXzg0yv78xWJ2ou6macUHGmpvUjjP88oHuic5y8Efht4UETur8n+XFVv7aNOa5K+G+cYSyTxkmFDAEYNKaOAIj12bYhokzHLWq0rqB6RPt3e9jmnsyOJ8+d3D1X9Ibh8uZ2gr8ZZUSpSIZCAWMMl2/n45DyLquBr71S2Ws2bEWtzFjbCEqrgxR6xqU42sTPODoejDfpqnC2WSCJiomVXzoLBF7BoT+sMKnq2snbT16igtZ/VezgcjuVp3WW0VJrV5fC9rS21D5dI2bocsW39mqXoq3EOJeQMJwm1RBgnDxvUI6K9rS6iltAqxdgQWF128mhErIJVIbL0ZTMQ4P7o202185fOW9U1xsrNVVkvxUk31my4Niq0Oxyroa/GOZCAYjxJJcoT26U3OeZXy8skv+sKoSrl2BC26Di2KliqBtqtnB0ORzv02edsCW2paph16WKklqq/uZeba0pM3gZMVoaYiysYk8ZqGtWQpZwVihIpeC26QhyDj1iLKXfukbU5Gj+ttfroLJI85NEoLNQxWPTVOEdEhFGe2M6ynHe2asSrq1Hbq8rcGnHcO0E6v5fTMkPW30QgKYLoNKqN/WORxATWYLAYqW4oumgNh8PRDn2Pwq1m2FreglVD1Gp+3F4ZZ6CkM8xQpCBzwHwi7eWHzGp1vRPVfjrb7HA42qHvcc7NEEqFq9gx0wAACMZJREFU6dA7+3svUGImy08w508QRdUNS9Ww5tZojK8eKaMYYDb0qFihGPXPPFfCE021e2y29xVq/uroB5tqd9nmxeUr4SuH93RaHYdj4FgbxlkrFKPq5lqkvTHO/397Zxsi11nF8d9/XnY3r2uTrWlI0tRgBKNfrLH4hgTqB0lLKyqlghKwIFWEqh/aiGBBKESFoiCiAUt9o6a+0AbfIEZb8IO1aW0kSdGkxdiUNJvGNJtsupmZO8cP9246ztyZvTs7c192zw8ue+8zd/Y599y553nu85znHDAawTkawbl5fascTVrONMXlhqg10+vpO46zeCiEcb4UTHJs6gqGccm6ZsTKnFGqjFfDic1ztYoPaTiO0zdJMqFsAn5MGI/AgL1m9h1Ja4B9wA3Av4E7zOz8MIScmjnBk6XwFb3Z7CP+ZkqsUJV1y6doBCVeujwautS5hV4UWGWU+rVbYj4ZXoarbrE1eg2txfGl6z7TUfbg6e/1JZOTHkkmBGfTzmwD3kuYy24bsBs4aGZbgYPR8ZAIaDYv0mxeBILhVTMAShiSURbR1tutTtJDkiYlHWkpWyPpgKTj0d9rhi644zi5Ikmaqm5pZ24nzC0I8CPgCeC+oUhZEGYs4PyVMSRjvBpQLRmX6mXo3dF5GPgu4dvJLLMN3x5Ju6Pjoel2z6n89qJOTo13lL1tdcy8w5kUhHGcFJmXK11b2pl1keEGeIVw2CPuO5+VdEjSoQaL2/E9oMl0o0ItKDNWDlhVCRidI4helH2j3SXhdsIGj+jvRwcurOM4uSbxhGB72hm1vK6bmanL2moz2wvsBVhRmljUI7ANAurNUKUzQZl6U/TpSZeo4XOcxUBpbIKRt3eOi/fmyXnXE65TSE5J84/h8sDmj8/7O/e9GO9Wmsg4d0k7c0bSejM7LWk9kF83ipQIaDITlAhMYcCkQFxZ4BB5r4avNQ/biNL3VV5KlEbGWbbxlo7ySvlA7PmNYOFz49YlpEG3mOYj1TfHlq9b1j00gpNf5mxKeqSd2Q/sivZ3AY8PXrziUm+Kugnrz6HuTNTg0avha83DVsGTZDrOYiJJzzk27QywB3hU0l3ASeCO4YhYHISijC0w3RDTDXG50dcilNmGbw9LvOGrxYQM/ft/RzOQxJkPksqEfoYvm9mtWctTRJJ4a/RKO3PzYMUpNiVEWWG0kFpTzARh2NFeSHqE0OtlQtIp4H684XOKzz3A88DqrAUpKoVYIVg0zERgUG+CzWGczeyTXT7yhs8pJJI2ArcADwBfzlicwpJ5VLrFSqMJgVkU5nRRO6k4TjvfBu6lW0Bq/t/F9uzZqfQkKxCZ9pwrVKiWV1EHrPk6RvFnlcNs3VBSuEE43OEGOn0kPQTcCkya2Tujsr7CDgSNaabPdy7V/vBY/IjTH6Z/0FH2sdWfiz33L8EzseUric95d7p+NLa81rgQW75heWeWobeu6PQ8ATgx/dvY8qRImtX3M5J2dDuv1cV2+/Yt/nDEkKlxXm4r2Tzybmqq8UrtKDO1U1mKs2AqlBktNQlMjJVhJhDVOZZvD5P3LPt0R9nTr/8kA0n6Z1W1c3llOSazRxceJuPVl0uQDwC3SdoJjAGrJf3UzD6VsVyFI9NhjapVGG++iTXBWqqlZVmKMhCE3ug5E8bXKGVonJc6vvoyfczsK2a20cxuAO4E/uSGuT98QnCATJbOcuS16ykLphvhuHNjjglBJ3USr75sXeSzaeOqFERznDfwCcEB8qr9h8MXZjh2oc5rtTDZ61zeGk52WHhzut6g1kU+a9cW/80ubczsCfdx7h83zgOkaQEBTQKf/MsziVZfOk7WKM2enaSzwDTwamqVDocJ+ruGzWZ27aCFgau6PRkd9itfnpjvNcTqNoqk+JsWb41vAedaJgTXmNm9c/3zFv0uBt0mZfZah/a7hY7fblz9WZFW/fG/3bRfuyUdMrPtqVY6YPJ+DXmXLwmDuIbW1ZeEEZ/vBx4DHgWuJ1p9aWadWWSHKFdRyPpal3r9PiHoLFp89aVTZHzM2XEcJ4dkYZz3ZlDnoMn7NeRdviTk9RryKtcwyPpal3T9qY85O47jOHPjwxqO4zg5JFXjLOkjkv4p6UTkxpR7JG2S9GdJxyQdlXRPVL5G0gFJx6O/1+RA1sLpF8IARZImJR1pKXP9pkTW+p9Lr5JGJe2LPn8qco8cVN2xz3fbOTskXZD0XLR9bVD198TMUtmAMvACsAUYAQ4D29KqfwFyrwdujPZXAf8CtgHfBHZH5buBb2QsZyH1G8n+IeBG4EhLmet3Ceg/iV6BzwPfj/bvBPYNsP7Y57vtnB2EvvKp3pc0e843ASfM7EUzqwE/JwxCk2vM7LSZPRvtXyTM7rCB/AXQKaR+oTABigqr37nIWP9J9Noqyy+Bm6Pcpgumx/OdOWka5w3ASy3Hp8iJEpISvU69C3iKeQTQSYnC67cN12+2pKX/JHq9eo6FKckvQJdg1wug7flu532SDkv6vaR3DLruOHwRSkIkrQR+BXzRzKZaG24zM0nu9jIkXL/ZshT03/58t338LOES60tRnOrHgK3DlinNnvPLwKaW441RWe6RVCW8cT8zs19HxXkLoFNY/XbB9Zstaek/iV6vniOpAowD5wYlQJfn+ypmNmVml6L93wFVSRODqr8baRrnp4Gtkt4iaYRwYH9/ivX3RTS29UPgeTN7sOWj/cCuaH8X8HjasrVRSP32wPWbLWnpP4leW2X5BGEA/4H05Hs8363nXDc7xi3pJkK7ObDGoStpzj4COwlnQ18Avpr27GefMn+QMObvP4Dnom0n4ZjXQeA48EfC6GZZy1o4/UZyPwKcBuqEY453uX6Xjv7j9Ap8Hbgt2h8DfgGcAP4GbBlg3d2e77uBu6NzvgAcJfQk+Svw/jTui68QdBzHySG+QtBxHCeHuHF2HMfJIW6cHcdxcogbZ8dxnBzixtlxHCeHuHF2HMfJIW6cHcdxcogbZ8dxnBzyP68jFtpWsUQoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLFeXivS0t8I",
        "colab_type": "text"
      },
      "source": [
        " **Human vs Horse classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNtswkyC06x8",
        "colab_type": "text"
      },
      "source": [
        "loading zip file, ImageDatagenerator, fig_generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px8lWEZ01Hm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "cabf3562-ea33-402e-848d-3147b67e2bf9"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip,'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-02 11:42:28--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.26.128, 2607:f8b0:400c:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.26.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: /tmp/horse-or-human.zip\n",
            "\n",
            "/tmp/horse-or-human 100%[===================>] 142.65M   213MB/s    in 0.7s    \n",
            "\n",
            "2020-06-02 11:42:29 (213 MB/s) - /tmp/horse-or-human.zip saved [149574867/149574867]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G5uDrez1wC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ce712433-c8ea-4c8d-942a-5b385747717e"
      },
      "source": [
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "\n",
        "print(len(os.listdir(train_horse_dir)))\n",
        "print(len(os.listdir(train_human_dir)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnmj3P4h2xtI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "2226959c-5951-494c-c4d8-37aacd92044b"
      },
      "source": [
        "model=tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6xJy8sD29L1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axfDbHJg3FGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d09eb609-5004-4bcd-c0f5-012a70df0256"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/tmp/horse-or-human',\n",
        "                                                    target_size=(300,300),\n",
        "                                                    batch_size=128,\n",
        "                                                    class_mode='binary')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo99Wq-C4KsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "6e00e53b-fcdc-4aaa-d8d0-882b01d06595"
      },
      "source": [
        "history=model.fit_generator (train_generator,steps_per_epoch=8,epochs=10,verbose=1)\n",
        "\n",
        "history.history['accuracy'][-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-31-4c0430d168a3>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/10\n",
            "8/8 [==============================] - 63s 8s/step - loss: 5.9497 - accuracy: 0.5139\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 62s 8s/step - loss: 0.6731 - accuracy: 0.6329\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 63s 8s/step - loss: 0.6911 - accuracy: 0.6385\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 64s 8s/step - loss: 0.5897 - accuracy: 0.7664\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 62s 8s/step - loss: 0.9915 - accuracy: 0.7341\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 62s 8s/step - loss: 0.5076 - accuracy: 0.8076\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 72s 9s/step - loss: 0.4598 - accuracy: 0.8350\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 63s 8s/step - loss: 0.6063 - accuracy: 0.8721\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 73s 9s/step - loss: 0.1652 - accuracy: 0.9433\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 63s 8s/step - loss: 0.1112 - accuracy: 0.9611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4c0430d168a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'acc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_fuwZer5uLZ",
        "colab_type": "text"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXyStaAl5W2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeK8e7PtFyFu",
        "colab_type": "text"
      },
      "source": [
        "**Cat vs Dog  Binary Classification** *Validation data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1BEf1GcFxpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "ab8c7426-a84f-488e-82b7-4f6f80da9add"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "  -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-02 12:55:40--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.13.128, 2607:f8b0:400c:c13::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.13.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: /tmp/cats_and_dogs_filtered.zip\n",
            "\n",
            "\r          /tmp/cats   0%[                    ]       0  --.-KB/s               \r         /tmp/cats_  34%[=====>              ]  22.45M   112MB/s               \r/tmp/cats_and_dogs_ 100%[===================>]  65.43M   191MB/s    in 0.3s    \n",
            "\n",
            "2020-06-02 12:55:41 (191 MB/s) - /tmp/cats_and_dogs_filtered.zip saved [68606236/68606236]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UveJYofGhr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat/dog pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat/dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcKfxzFQGqT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "648bc42a-4cbd-4435-d2bc-30bda4cf4107"
      },
      "source": [
        "train_cat_fnames = os.listdir( train_cats_dir )\n",
        "train_dog_fnames = os.listdir( train_dogs_dir )\n",
        "\n",
        "print('total training cat images :', len(os.listdir(      train_cats_dir ) ))\n",
        "print('total training dog images :', len(os.listdir(      train_dogs_dir ) ))\n",
        "\n",
        "print('total validation cat images :', len(os.listdir( validation_cats_dir ) ))\n",
        "print('total validation dog images :', len(os.listdir( validation_dogs_dir ) ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training cat images : 1000\n",
            "total training dog images : 1000\n",
            "total validation cat images : 500\n",
            "total validation dog images : 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdrN-WrnG19T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "606cd0be-b193-4b4d-bb99-2120429ef0e1"
      },
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2), \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(), \n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'), \n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 148, 148, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 74, 74, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 72, 72, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 36, 36, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 34, 34, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 18496)             0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 512)               9470464   \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 9,494,561\n",
            "Trainable params: 9,494,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB20KWMjHBAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYdzDvCxHIXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6f50d8f4-7339-4f9e-c1fe-ef5b0dc43729"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255.\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "# --------------------\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))     \n",
        "# --------------------\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
        "                                                         batch_size=20,\n",
        "                                                         class_mode  = 'binary',\n",
        "                                                         target_size = (150, 150))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWOr2T8cHZ1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "43aaa0dd-6f77-4278-935b-cc19a2529c14"
      },
      "source": [
        "history = model.fit(train_generator,\n",
        "                              validation_data=validation_generator,\n",
        "                              steps_per_epoch=100,\n",
        "                              epochs=15,\n",
        "                              validation_steps=50,\n",
        "                              verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "100/100 - 56s - loss: 0.8556 - accuracy: 0.5575 - val_loss: 0.6473 - val_accuracy: 0.5720\n",
            "Epoch 2/15\n",
            "100/100 - 56s - loss: 0.6473 - accuracy: 0.6600 - val_loss: 0.5770 - val_accuracy: 0.7010\n",
            "Epoch 3/15\n",
            "100/100 - 56s - loss: 0.5591 - accuracy: 0.7320 - val_loss: 0.6265 - val_accuracy: 0.6590\n",
            "Epoch 4/15\n",
            "100/100 - 57s - loss: 0.4730 - accuracy: 0.7740 - val_loss: 0.6015 - val_accuracy: 0.6980\n",
            "Epoch 5/15\n",
            "100/100 - 58s - loss: 0.4032 - accuracy: 0.8190 - val_loss: 0.6834 - val_accuracy: 0.6750\n",
            "Epoch 6/15\n",
            "100/100 - 57s - loss: 0.3042 - accuracy: 0.8620 - val_loss: 0.8222 - val_accuracy: 0.6990\n",
            "Epoch 7/15\n",
            "100/100 - 57s - loss: 0.2188 - accuracy: 0.9115 - val_loss: 0.7180 - val_accuracy: 0.7130\n",
            "Epoch 8/15\n",
            "100/100 - 57s - loss: 0.1578 - accuracy: 0.9365 - val_loss: 0.9686 - val_accuracy: 0.7300\n",
            "Epoch 9/15\n",
            "100/100 - 57s - loss: 0.1143 - accuracy: 0.9565 - val_loss: 1.0340 - val_accuracy: 0.7200\n",
            "Epoch 10/15\n",
            "100/100 - 57s - loss: 0.1016 - accuracy: 0.9625 - val_loss: 1.2031 - val_accuracy: 0.7100\n",
            "Epoch 11/15\n",
            "100/100 - 57s - loss: 0.0955 - accuracy: 0.9785 - val_loss: 1.2957 - val_accuracy: 0.7350\n",
            "Epoch 12/15\n",
            "100/100 - 57s - loss: 0.1137 - accuracy: 0.9820 - val_loss: 1.3910 - val_accuracy: 0.7340\n",
            "Epoch 13/15\n",
            "100/100 - 57s - loss: 0.0340 - accuracy: 0.9890 - val_loss: 2.4432 - val_accuracy: 0.6770\n",
            "Epoch 14/15\n",
            "100/100 - 56s - loss: 0.0624 - accuracy: 0.9860 - val_loss: 1.7111 - val_accuracy: 0.7070\n",
            "Epoch 15/15\n",
            "100/100 - 56s - loss: 0.1202 - accuracy: 0.9865 - val_loss: 1.7133 - val_accuracy: 0.7050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDQ1fa1QHhvs",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "049f701c-6066-4ee1-8920-1906170ca8e9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=image.load_img(path, target_size=(150, 150))\n",
        "  \n",
        "  x=image.img_to_array(img)\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  \n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  print(classes[0])\n",
        "  \n",
        "  if classes[0]>0:\n",
        "    print(fn + \" is a dog\")\n",
        "    \n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-39a64c85-4416-4a50-b9cd-4572edf3ce2e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-39a64c85-4416-4a50-b9cd-4572edf3ce2e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlJHo5O6Hly4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "18fc6db9-4ad5-45b6-8f43-3792fc5d034d"
      },
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc      = history.history[     'accuracy' ]\n",
        "val_acc  = history.history[ 'val_accuracy' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     acc )\n",
        "plt.plot  ( epochs, val_acc )\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     loss )\n",
        "plt.plot  ( epochs, val_loss )\n",
        "plt.title ('Training and validation loss'   )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training and validation loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY7jS9H1Wznh",
        "colab_type": "text"
      },
      "source": [
        "**Cat vs dog data source and spliting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC4nC5FtWyoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated\n",
        "# ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position.\n",
        "# ATTENTION: Please use the provided epoch values when training.\n",
        "\n",
        "# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n",
        "# This will require you doing a lot of data preprocessing because\n",
        "# the dataset isn't split into training and validation for you\n",
        "# This code block has all the required inputs\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "from os import getcwd\n",
        "\n",
        "path_cats_and_dogs = f\"{getcwd()}/../tmp2/cats-and-dogs.zip\"\n",
        "shutil.rmtree('/tmp')\n",
        "\n",
        "local_zip = path_cats_and_dogs\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "\n",
        "print(len(os.listdir('/tmp/PetImages/Cat/')))\n",
        "print(len(os.listdir('/tmp/PetImages/Dog/')))\n",
        "\n",
        "# Use os.mkdir to create your directories\n",
        "# You will need a directory for cats-v-dogs, and subdirectories for training\n",
        "# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\n",
        "try:\n",
        "    \n",
        "    os.mkdir('/tmp/cats-v-dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
        "    #YOUR CODE GOES HERE\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "# Write a python function called split_data which takes\n",
        "# a SOURCE directory containing the files\n",
        "# a TRAINING directory that a portion of the files will be copied to\n",
        "# a TESTING directory that a portion of the files will be copie to\n",
        "# a SPLIT SIZE to determine the portion\n",
        "# The files should also be randomized, so that the training set is a random\n",
        "# X% of the files, and the test set is the remaining files\n",
        "# SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9\n",
        "# Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir\n",
        "# and 10% of the images will be copied to the TESTING dir\n",
        "# Also -- All images should be checked, and if they have a zero file length,\n",
        "# they will not be copied over\n",
        "#\n",
        "# os.listdir(DIRECTORY) gives you a listing of the contents of that directory\n",
        "# os.path.getsize(PATH) gives you the size of the file\n",
        "# copyfile(source, destination) copies a file from source to destination\n",
        "# random.sample(list, len(list)) shuffles a list\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "# YOUR CODE STARTS HERE\n",
        "\n",
        "    dataset=[]\n",
        "    for Soudata in os.listdir(SOURCE):\n",
        "        data=SOURCE+Soudata\n",
        "        if(os.path.getsize(data)>0):\n",
        "            dataset.append(Soudata)\n",
        "        else:\n",
        "            print('Skipped'+Soudata)\n",
        "            print('Invalid file size!')\n",
        "    train_data_length= int(len(dataset)* SPLIT_SIZE)\n",
        "    test_data_lenth=int(len(dataset)-train_data_length)\n",
        "    shuffled_set= random.sample(dataset,len(dataset))\n",
        "    train_set= shuffled_set[0:train_data_length]\n",
        "    test_set=shuffled_set[-test_data_lenth:]\n",
        "    \n",
        "    for unitData in train_set:\n",
        "        temp_train_data = SOURCE + unitData\n",
        "        final_train_data = TRAINING + unitData\n",
        "        copyfile(temp_train_data, final_train_data)\n",
        "    \n",
        "    for unitData in test_set:\n",
        "        temp_test_data = SOURCE + unitData\n",
        "        final_test_data = TESTING + unitData\n",
        "        copyfile(temp_train_data, final_test_data)\n",
        "            \n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
        "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
        "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
        "\n",
        "split_size = .9\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n",
        "print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n",
        "\n",
        "# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "# USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "# YOUR CODE HERE\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "TRAINING_DIR =\"/tmp/cats-v-dogs/training\" #YOUR CODE HERE\n",
        "train_datagen =ImageDataGenerator(rescale=1.0/255) #YOUR CODE HERE\n",
        "\n",
        "# NOTE: YOU MUST USE A BATCH SIZE OF 10 (batch_size=10) FOR THE \n",
        "# TRAIN GENERATOR.\n",
        "train_generator =train_datagen.flow_from_directory(TRAINING_DIR,batch_size=10,class_mode='binary', \n",
        "                                                    target_size=(150, 150)) #YOUR CODE HERE\n",
        "\n",
        "VALIDATION_DIR = \"/tmp/cats-v-dogs/testing\"\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# NOTE: YOU MUST USE A BACTH SIZE OF 10 (batch_size=10) FOR THE \n",
        "# VALIDATION GENERATOR.\n",
        "validation_generator =validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                             batch_size=10,\n",
        "                                                             class_mode='binary',\n",
        "                                                             target_size=(150,150)) #YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "# Expected Output:\n",
        "# Found 2700 images belonging to 2 classes.\n",
        "# Found 300 images belonging to 2 classes.\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=2,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator)\n",
        "\n",
        "\n",
        "# PLOT LOSS AND ACCURACY\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "\n",
        "\n",
        "plt.title('Training and validation loss')\n",
        "\n",
        "# Desired output. Charts with training and validation metrics. No crash :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCkNDaVrXSyK",
        "colab_type": "text"
      },
      "source": [
        "Image Augmentation with cat vs dog data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hW9e8nlemPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
        "# the image, we also rotate and do other operations\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9FNAJNNerKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiA2y2kbe6Dy",
        "colab_type": "text"
      },
      "source": [
        "Image Augmentation with cat vs dog data set with drop out to avoid overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6pp2DJHfFHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
        "# the image, we also rotate and do other operations\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV6ZTWJ1_7Mo",
        "colab_type": "text"
      },
      "source": [
        "Transfer Learning Cat vs Dog "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUr6jJfaZFLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEgi5Qs2ZduM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (1, activation='sigmoid')(x)           \n",
        "\n",
        "model = Model( pre_trained_model.input, x) \n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO_H-aJNc2p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "        https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "       -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '//tmp/cats_and_dogs_filtered.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir = os.path.join( base_dir, 'train')\n",
        "validation_dir = os.path.join( base_dir, 'validation')\n",
        "\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats') # Directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs') # Directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats') # Directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')# Directory with our validation dog pictures\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = 20,\n",
        "                                                    class_mode = 'binary', \n",
        "                                                    target_size = (150, 150))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = 20,\n",
        "                                                          class_mode  = 'binary', \n",
        "                                                          target_size = (150, 150))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xpY9CBZdCKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "            train_generator,\n",
        "            validation_data = validation_generator,\n",
        "            steps_per_epoch = 100,\n",
        "            epochs = 20,\n",
        "            validation_steps = 50,\n",
        "            verbose = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf-HsVUodFl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQYH3f8Eeptm",
        "colab_type": "text"
      },
      "source": [
        "Transfer Learning Human vs Horse "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1sAHco4eoGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated\n",
        "# ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position.\n",
        "# ATTENTION: Please use the provided epoch values when training.\n",
        "\n",
        "# Import all the necessary files!\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from os import getcwd\n",
        "\n",
        "path_inception = f\"{getcwd()}/../tmp2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "# Import the inception model  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Create an instance of the inception model from the local pre-trained weights\n",
        "local_weights_file = path_inception\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape=(150,150,3),include_top=False,weights=None)# Your Code Here\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# Make all the layers in the pre-trained model non-trainable\n",
        "for layer in pre_trained_model.layers:\n",
        "    layer.trainable=False\n",
        "  # Your Code Here\n",
        "\n",
        "  \n",
        "# Print the model summary\n",
        "pre_trained_model.summary()\n",
        "\n",
        "# Expected Output is extremely large, but should end with:\n",
        "\n",
        "#batch_normalization_v1_281 (Bat (None, 3, 3, 192)    576         conv2d_281[0][0]                 \n",
        "#__________________________________________________________________________________________________\n",
        "#activation_273 (Activation)     (None, 3, 3, 320)    0           batch_normalization_v1_273[0][0] \n",
        "#__________________________________________________________________________________________________\n",
        "#mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_275[0][0]             \n",
        "#                                                                 activation_276[0][0]             \n",
        "#__________________________________________________________________________________________________\n",
        "#concatenate_5 (Concatenate)     (None, 3, 3, 768)    0           activation_279[0][0]             \n",
        "#                                                                 activation_280[0][0]             \n",
        "#__________________________________________________________________________________________________\n",
        "#activation_281 (Activation)     (None, 3, 3, 192)    0           batch_normalization_v1_281[0][0] \n",
        "#__________________________________________________________________________________________________\n",
        "#mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_273[0][0]             \n",
        "#                                                                 mixed9_1[0][0]                   \n",
        "#                                                                 concatenate_5[0][0]              \n",
        "#                                                                 activation_281[0][0]             \n",
        "#==================================================================================================\n",
        "#Total params: 21,802,784\n",
        "#Trainable params: 0\n",
        "#Non-trainable params: 21,802,784\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')# Your Code Here)\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output =last_layer.output # Your Code Here\n",
        "\n",
        "# Expected Output:\n",
        "# ('last layer output shape: ', (None, 7, 7, 768))\n",
        "\n",
        "# Define a Callback class that stops training once accuracy reaches 97.0%\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.97):\n",
        "      print(\"\\nReached 97.0% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "      \n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024,activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (1,activation='sigmoid')(x)           \n",
        "\n",
        "model = Model(pre_trained_model.input , x) \n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Expected output will be large. Last few lines should be:\n",
        "\n",
        "# mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_248[0][0]             \n",
        "#                                                                  activation_251[0][0]             \n",
        "#                                                                  activation_256[0][0]             \n",
        "#                                                                  activation_257[0][0]             \n",
        "# __________________________________________________________________________________________________\n",
        "# flatten_4 (Flatten)             (None, 37632)        0           mixed7[0][0]                     \n",
        "# __________________________________________________________________________________________________\n",
        "# dense_8 (Dense)                 (None, 1024)         38536192    flatten_4[0][0]                  \n",
        "# __________________________________________________________________________________________________\n",
        "# dropout_4 (Dropout)             (None, 1024)         0           dense_8[0][0]                    \n",
        "# __________________________________________________________________________________________________\n",
        "# dense_9 (Dense)                 (None, 1)            1025        dropout_4[0][0]                  \n",
        "# ==================================================================================================\n",
        "# Total params: 47,512,481\n",
        "# Trainable params: 38,537,217\n",
        "# Non-trainable params: 8,975,264\n",
        "\n",
        "\n",
        "# Get the Horse or Human dataset\n",
        "path_horse_or_human = f\"{getcwd()}/../tmp2/horse-or-human.zip\"\n",
        "# Get the Horse or Human Validation dataset\n",
        "path_validation_horse_or_human = f\"{getcwd()}/../tmp2/validation-horse-or-human.zip\"\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/tmp')\n",
        "local_zip = path_horse_or_human\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/training')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = path_validation_horse_or_human\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "train_dir = '/tmp/training'\n",
        "validation_dir = '/tmp/validation'\n",
        "\n",
        "train_horses_dir = '/tmp/training/horses'# Your Code Here\n",
        "train_humans_dir = '/tmp/training/humans'# Your Code Here\n",
        "validation_horses_dir = '/tmp/validation/horses'# Your Code Here\n",
        "validation_humans_dir ='/tmp/validation/humans' # Your Code Here\n",
        "\n",
        "train_horses_fnames = os.listdir(train_horses_dir)# Your Code Here\n",
        "train_humans_fnames = os.listdir(train_humans_dir)# Your Code Here\n",
        "validation_horses_fnames = os.listdir(validation_horses_dir)# Your Code Here\n",
        "validation_humans_fnames = os.listdir(validation_humans_dir)# Your Code Here\n",
        "\n",
        "print(len(os.listdir(train_horses_dir)))\n",
        "print(len(os.listdir(train_humans_dir)))\n",
        "print(len(os.listdir(validation_horses_dir)))\n",
        "print(len(os.listdir(validation_humans_dir)))\n",
        "\n",
        "# Expected Output:\n",
        "# 500\n",
        "# 527\n",
        "# 128\n",
        "# 128\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = 10,\n",
        "                                                    class_mode = 'binary', \n",
        "                                                    target_size = (150, 150))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = 10,\n",
        "                                                          class_mode  = 'binary', \n",
        "                                                          target_size = (150, 150))\n",
        "\n",
        "# Expected Output:\n",
        "# Found 1027 images belonging to 2 classes.\n",
        "# Found 256 images belonging to 2 classes.\n",
        "\n",
        "# Run this and see how many epochs it should take before the callback\n",
        "# fires, and stops training at 97% accuracy\n",
        "\n",
        "callbacks = myCallback()\n",
        "history = model.fit_generator(train_generator,\n",
        "            validation_data = validation_generator,\n",
        "            #steps_per_epoch = 100,\n",
        "            epochs = 3,\n",
        "            #validation_steps = 50,\n",
        "            verbose = 2,callbacks=[callbacks])\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkHUF3Aq53sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clean_up\n",
        "\n",
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}